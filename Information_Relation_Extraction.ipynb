{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHfQMjDnlACs"
      },
      "source": [
        "# Assignment 3 - CT5120/CT5146\n",
        "\n",
        "### Instructions:\n",
        "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **December 31, 2021**. Please note that there will be no further extensions to this deadline and we highly encourage you to submit this assignment before Semester 1 exams.\n",
        "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
        "- The assignment is worth $100$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
        "\n",
        "|           | Task | Marks |\n",
        "| :---      | :-----| -----:|\n",
        "| Task 1    | Pre-processing |   15 |\n",
        "| Task 2    | Named Entity Recognition |    10 |\n",
        "| Task 3    | Information / Relation Extraction (I) | 30 |\n",
        "| Task 4    | Information / Relation Extraction (II) | 15 |\n",
        "| Task 5    | Combining information in the output   | 5 |\n",
        "| Task 6    | Evaluation (I) | 15 |\n",
        "| Task 7    | Evaluation (II) | 10 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEpjx2qKnyBl"
      },
      "source": [
        "---\n",
        "\n",
        "## Information Extraction and Relation Extraction\n",
        "\n",
        "In the following tasks you will write code to perform **_information extraction_** and **_relation extraction_** across a collection of documents in `movies.zip`.\n",
        "\n",
        "The zip archive contains 100 files, out of which 50 are plaintext documents and other 50 contain data structured as JSON.\n",
        "Each plaintext document contains a text description of a movie taken from the English version of Wikipedia, while each JSON document contains *gold-standard* labels (also called *reference* labels) stored as key-value pairs for the entities and relations for each document.\n",
        "\n",
        "You are only allowed to use the given documents and labels and **must not** use any other external sources of data for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64euw8hXygah"
      },
      "source": [
        "---\n",
        "\n",
        "Download and unarchive `movies.zip` from Blackboard and place it in the same location as this notebook or uncomment the code cell below to get the data in a directory called `movies` and also place it automatically in the same location as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OXzoZVNZyevs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f382b9d-3a6d-444e-b8d7-f0cc30a67497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-02 22:23:10--  https://drive.google.com/uc?export=download&id=1L6NcSGkubNJaL6xSnYEZZKSrlyXq1AbB\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.195.100, 74.125.195.139, 74.125.195.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.195.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/iot4l6711uig94ftv8qlv1cr7mtatqfh/1641162150000/04741348677416923358/*/1L6NcSGkubNJaL6xSnYEZZKSrlyXq1AbB?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-01-02 22:23:11--  https://doc-04-90-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/iot4l6711uig94ftv8qlv1cr7mtatqfh/1641162150000/04741348677416923358/*/1L6NcSGkubNJaL6xSnYEZZKSrlyXq1AbB?e=download\n",
            "Resolving doc-04-90-docs.googleusercontent.com (doc-04-90-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-04-90-docs.googleusercontent.com (doc-04-90-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94432 (92K) [application/zip]\n",
            "Saving to: ‘movies.zip’\n",
            "\n",
            "movies.zip          100%[===================>]  92.22K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-01-02 22:23:11 (143 MB/s) - ‘movies.zip’ saved [94432/94432]\n",
            "\n",
            "Archive:  movies.zip\n",
            "   creating: movies/\n",
            "  inflating: movies/50.doc.txt       \n",
            "  inflating: movies/40.doc.txt       \n",
            "  inflating: movies/32.doc.txt       \n",
            "  inflating: movies/22.doc.txt       \n",
            "  inflating: movies/14.info.json     \n",
            "  inflating: movies/35.info.json     \n",
            "  inflating: movies/21.info.json     \n",
            "  inflating: movies/19.info.json     \n",
            "  inflating: movies/49.doc.txt       \n",
            "  inflating: movies/42.info.json     \n",
            "  inflating: movies/38.info.json     \n",
            "  inflating: movies/14.doc.txt       \n",
            "  inflating: movies/04.doc.txt       \n",
            "  inflating: movies/33.info.json     \n",
            "  inflating: movies/49.info.json     \n",
            "  inflating: movies/48.doc.txt       \n",
            "  inflating: movies/27.info.json     \n",
            "  inflating: movies/05.doc.txt       \n",
            "  inflating: movies/15.doc.txt       \n",
            "  inflating: movies/06.info.json     \n",
            "  inflating: movies/12.info.json     \n",
            "  inflating: movies/41.doc.txt       \n",
            "  inflating: movies/44.info.json     \n",
            "  inflating: movies/50.info.json     \n",
            "  inflating: movies/23.doc.txt       \n",
            "  inflating: movies/33.doc.txt       \n",
            "  inflating: movies/15.info.json     \n",
            "  inflating: movies/01.info.json     \n",
            "  inflating: movies/28.doc.txt       \n",
            "  inflating: movies/38.doc.txt       \n",
            "  inflating: movies/20.info.json     \n",
            "  inflating: movies/17.doc.txt       \n",
            "  inflating: movies/07.doc.txt       \n",
            "  inflating: movies/34.info.json     \n",
            "  inflating: movies/18.info.json     \n",
            "  inflating: movies/31.doc.txt       \n",
            "  inflating: movies/21.doc.txt       \n",
            "  inflating: movies/43.doc.txt       \n",
            "  inflating: movies/43.info.json     \n",
            "  inflating: movies/39.info.json     \n",
            "  inflating: movies/20.doc.txt       \n",
            "  inflating: movies/30.doc.txt       \n",
            "  inflating: movies/26.info.json     \n",
            "  inflating: movies/32.info.json     \n",
            "  inflating: movies/42.doc.txt       \n",
            "  inflating: movies/48.info.json     \n",
            "  inflating: movies/13.info.json     \n",
            "  inflating: movies/07.info.json     \n",
            "  inflating: movies/39.doc.txt       \n",
            "  inflating: movies/29.doc.txt       \n",
            "  inflating: movies/45.info.json     \n",
            "  inflating: movies/06.doc.txt       \n",
            "  inflating: movies/16.doc.txt       \n",
            "  inflating: movies/37.info.json     \n",
            "  inflating: movies/23.info.json     \n",
            "  inflating: movies/13.doc.txt       \n",
            "  inflating: movies/03.doc.txt       \n",
            "  inflating: movies/02.info.json     \n",
            "  inflating: movies/16.info.json     \n",
            "  inflating: movies/40.info.json     \n",
            "  inflating: movies/35.doc.txt       \n",
            "  inflating: movies/25.doc.txt       \n",
            "  inflating: movies/47.doc.txt       \n",
            "  inflating: movies/04.info.json     \n",
            "  inflating: movies/10.info.json     \n",
            "  inflating: movies/31.info.json     \n",
            "  inflating: movies/24.doc.txt       \n",
            "  inflating: movies/34.doc.txt       \n",
            "  inflating: movies/46.doc.txt       \n",
            "  inflating: movies/25.info.json     \n",
            "  inflating: movies/09.info.json     \n",
            "  inflating: movies/02.doc.txt       \n",
            "  inflating: movies/12.doc.txt       \n",
            "  inflating: movies/46.info.json     \n",
            "  inflating: movies/28.info.json     \n",
            "  inflating: movies/22.info.json     \n",
            "  inflating: movies/09.doc.txt       \n",
            "  inflating: movies/19.doc.txt       \n",
            "  inflating: movies/36.info.json     \n",
            "  inflating: movies/44.doc.txt       \n",
            "  inflating: movies/17.info.json     \n",
            "  inflating: movies/03.info.json     \n",
            "  inflating: movies/36.doc.txt       \n",
            "  inflating: movies/26.doc.txt       \n",
            "  inflating: movies/10.doc.txt       \n",
            "  inflating: movies/41.info.json     \n",
            "  inflating: movies/11.info.json     \n",
            "  inflating: movies/01.doc.txt       \n",
            "  inflating: movies/11.doc.txt       \n",
            "  inflating: movies/05.info.json     \n",
            "  inflating: movies/24.info.json     \n",
            "  inflating: movies/30.info.json     \n",
            "  inflating: movies/18.doc.txt       \n",
            "  inflating: movies/08.info.json     \n",
            "  inflating: movies/08.doc.txt       \n",
            "  inflating: movies/29.info.json     \n",
            "  inflating: movies/45.doc.txt       \n",
            "  inflating: movies/27.doc.txt       \n",
            "  inflating: movies/37.doc.txt       \n",
            "  inflating: movies/47.info.json     \n"
          ]
        }
      ],
      "source": [
        "!if test -f \"movies.zip\"; then rm \"movies.zip\"; fi\n",
        "!if test -d \"movies/\"; then rm -rf \"movies/\"; fi\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1L6NcSGkubNJaL6xSnYEZZKSrlyXq1AbB\" -O \"movies.zip\"\n",
        "!unzip \"movies.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm1emJILzF5K"
      },
      "source": [
        "---\n",
        "\n",
        "## Reading Data\n",
        "\n",
        "Place the unzipped `movies` directory in the same location as this notebook and run the following code cell to read the plaintext and JSON documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "P5hzct-HzUvJ"
      },
      "outputs": [],
      "source": [
        "######### DO NOT EDIT THIS CELL #########\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "documents = []   # store the text documents as a list of strings\n",
        "labels = []      # store the gold-standard labels as a list of dictionaries\n",
        "\n",
        "for idx in range(50):\n",
        "  with open(os.path.join('movies', str(idx+1).zfill(2) + '.doc.txt')) as f:\n",
        "    doc = f.read().strip()\n",
        "  with open(os.path.join('movies', str(idx+1).zfill(2) + '.info.json')) as f:\n",
        "    label = json.load(f)\n",
        "\n",
        "  documents.append(doc)\n",
        "  labels.append(label)\n",
        "\n",
        "assert len(documents) == 50\n",
        "assert len(labels) == 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnmnLhDyj2eG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "nPCKvyYFj0zG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1559bc3d-0f23-4a78-e340-f4646cee7c23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# Load the libraries which might be useful\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('all', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899-kd7LmlFp"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 1: Document Pre-processing (15 Marks)\n",
        "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
        "\n",
        "The expected output is a list of tagged sentences where each tagged sentence is a list containing `(token, tag)` pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dB8R43AklZxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc088122-127a-4c8a-99e2-55539d3d577e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " ('received', 'VBD'),\n",
              " ('ten', 'JJ'),\n",
              " ('Oscar', 'NNP'),\n",
              " ('nominations', 'NNS'),\n",
              " ('(', '('),\n",
              " ('including', 'VBG'),\n",
              " ('Best', 'NNP'),\n",
              " ('Picture', 'NN'),\n",
              " (')', ')'),\n",
              " (',', ','),\n",
              " ('winning', 'VBG'),\n",
              " ('seven', 'CD'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "def ie_preprocess(document):\n",
        "  '''Return a list of sentences tagged with part-of-speech tags for the given document.'''\n",
        "#Creating a list\n",
        "  tagged_sentences = []\n",
        "\n",
        "\n",
        "  #Sentence segmentation\n",
        "  sentences = nltk.sent_tokenize(document)\n",
        "  \n",
        "  #Sentence is split into tokens\n",
        "  #Token is a smallest unit of text or a sentence\n",
        "  #Tokenization is the process of splitting the text to extract tokens.\n",
        "  # Tokenizing the sentences into words using nltk.word_tokenize\n",
        "  tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "  \n",
        "  # POS tagging\n",
        "  \n",
        "  # Parts of Speech tagging helps to infer knowledge on how a word is used in a sentence or text\n",
        "  tagged_sentences = [nltk.pos_tag(sent) for sent in tokenized_sentences]\n",
        "  \n",
        "    \n",
        "  \n",
        "  return tagged_sentences\n",
        "  #output\n",
        "ie_preprocess(documents[0])[-10]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ie_preprocess(documents[0])[-10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SCbHzSex2a6",
        "outputId": "d0e0cb93-4659-48a3-fd97-ef02f5449cb8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " ('received', 'VBD'),\n",
              " ('ten', 'JJ'),\n",
              " ('Oscar', 'NNP'),\n",
              " ('nominations', 'NNS'),\n",
              " ('(', '('),\n",
              " ('including', 'VBG'),\n",
              " ('Best', 'NNP'),\n",
              " ('Picture', 'NN'),\n",
              " (')', ')'),\n",
              " (',', ','),\n",
              " ('winning', 'VBG'),\n",
              " ('seven', 'CD'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KJLD-AMMvdq"
      },
      "source": [
        "Run the cell below to check if the output is formatted correctly.\n",
        "\n",
        "Expected output: `[('It', 'PRP'), ('received', 'VBD'), ('ten', 'JJ'), ('Oscar', 'NNP'), ('nominations', 'NNS'), ('(', '('), ('including', 'VBG'), ('Best', 'NNP'), ('Picture', 'NN'), (')', ')'), (',', ','), ('winning', 'VBG'), ('seven', 'CD'), ('.', '.')]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "9sEQYa3TBDYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d4136f-51fa-4884-dc67-5b4cc56805ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('In', 'IN'),\n",
              " ('2004', 'CD'),\n",
              " (',', ','),\n",
              " ('its', 'PRP$'),\n",
              " ('soundtrack', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('added', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('U.S.', 'NNP'),\n",
              " ('National', 'NNP'),\n",
              " ('Recording', 'NNP'),\n",
              " ('Registry', 'NNP'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('was', 'VBD'),\n",
              " ('additionally', 'RB'),\n",
              " ('listed', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('American', 'NNP'),\n",
              " ('Film', 'NNP'),\n",
              " ('Institute', 'NNP'),\n",
              " ('as', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('best', 'JJS'),\n",
              " ('movie', 'NN'),\n",
              " ('score', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('all', 'DT'),\n",
              " ('time', 'NN'),\n",
              " ('a', 'DT'),\n",
              " ('year', 'NN'),\n",
              " ('later', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# check output for Task 1\n",
        "ie_preprocess(documents[0])[-7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgEPCLXmq8bC"
      },
      "source": [
        "## Task 2: Named Entity Recognition (10 Marks)\n",
        "\n",
        "Write a function that returns a list of all the named entities in a given document. The document here is structured as a list of sentences and tagged with part-of-speech tags.\n",
        "\n",
        "Hint: Set `binary = True` while calling the `ne_chunk` function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-porcess and POS tagged documents from the previous step is used in this function\n",
        "def find_named_entities(tagged_document):\n",
        "  '''Return a list of all the named entities in the given tagged document.'''\n",
        "\n",
        "  named_entities = []\n",
        "  #The each sentence present in the document is grouped in to trees\n",
        "  #Leaf Node in a tree can be defined as a node without any children\n",
        "  #Named Entity Recognition is a process of automatically extracting the Named Entities from a corpus of text \n",
        "  #Named Entities that are found in a leaf node are extracted from the trees are extracted and updated to a the \"named_entities\" list\n",
        "  #Looping each sentence in the tagged_document\n",
        "  #each sentence present in  the document is chunked into trees\n",
        "  #named enties present in  the leaf node of individual tree are filtered and are appended into a list\n",
        "  for sentu in tagged_document:\n",
        "        \n",
        "    #In unconstrained text, classes and interfaces are used to define linguistic groupings which are non-overlapping in nature. This process is called as \"Chunk Parsing\"\n",
        "    #The groups obtained from the process of Chunk Parsing is called as \"Chunks\"\n",
        "    tree = nltk.ne_chunk(sentu, binary=True)\n",
        "    for subtree in tree.subtrees():\n",
        "     #The label() is used to obtain a specific node by its label in a tree\n",
        "    #Checking for a Named Entity\n",
        "      if subtree.label() == 'NE':\n",
        "        entity = \"\"\n",
        "        for leaf in subtree.leaves():\n",
        "          entity = entity + leaf[0] + \" \"\n",
        "        named_entities.append(entity.strip())\n",
        "             \n",
        "\n",
        "  return named_entities"
      ],
      "metadata": {
        "id": "jVYEIZfUKVwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qHKjz5TKp5uM"
      },
      "outputs": [],
      "source": [
        "#pre-porcess and POS tagged documents from the previous step is used in this function\n",
        "def find_named_entities(tagged_document):\n",
        "  '''Return a list of all the named entities in the given tagged document.'''\n",
        "\n",
        "  named_entities = []\n",
        "  #each sentence present in  the document is chunked into trees\n",
        "  #named enties present in  the leaf node of individual tree are filtered and are appended into a list called \"named_entities\"\n",
        "  for sentu in tagged_document:\n",
        "    tree = nltk.ne_chunk(sentu, binary=True)\n",
        "    for subtree in tree.subtrees():\n",
        "      if subtree.label() == 'NE':\n",
        "        entity = \"\"\n",
        "        for leaf in subtree.leaves():\n",
        "          entity = entity + leaf[0] + \" \"\n",
        "        named_entities.append(entity.strip())\n",
        "             \n",
        "\n",
        "  return named_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvZNyV-ENDZc"
      },
      "source": [
        "Run the cell below to check if the output is formatted correctly.\n",
        "\n",
        "The output values might not match exactly, but should look similar to: `['Star Wars', 'Star Wars', 'New Hope', 'American', 'George Lucas', 'Lucasfilm', ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lnlqsKg7sk29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a9c79a-d7e0-40d1-df8d-48452b649815"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Star Wars',\n",
              " 'Star Wars',\n",
              " 'New Hope',\n",
              " 'American',\n",
              " 'George Lucas',\n",
              " 'Lucasfilm',\n",
              " 'Century Fox',\n",
              " 'Mark Hamill',\n",
              " 'Harrison Ford',\n",
              " 'Carrie Fisher']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# check output for Task 2\n",
        "tagged_document = ie_preprocess(documents[0]) # pre-process the first document\n",
        "find_named_entities(tagged_document)[:10]     # display the first 10 named entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmpuu0yvwI_X"
      },
      "source": [
        "## Task 3: Information / Relation Extraction (I) (30 Marks)\n",
        "\n",
        "Choose any **three** relations out of the following and write functions to extract them from a given document.\n",
        "\n",
        "* **Title**\n",
        "* **Language**\n",
        "* **Starring**\n",
        "* **Release date**\n",
        "* **Cinematography**\n",
        "* **Dialogue by**\n",
        "* **Directed by**\n",
        "* **Edited by**\n",
        "* **Music by**\n",
        "* **Narrated by**\n",
        "* **Produced by**\n",
        "* **Screenplay by**\n",
        "* **Story by**\n",
        "* **Written by**\n",
        "* **Production companies**\n",
        "* **Distribution companies**\n",
        "* **Budget**\n",
        "* **Box office**\n",
        "\n",
        "\n",
        "The functions you define here must take as input a string called `document` and return the information/relation extracted as a list. You can explain your approach with comments along with your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Yw8YQAr-wwFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cde1635-ea4b-45e0-fcd5-dee87629ee54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NE: 'George/NNP Lucas/NNP'] ',/, produced/VBN by/IN' [NE: 'Lucasfilm/NNP']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lucasfilm', 'george_lucas']"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "# relation 1 - your code goes here\n",
        "\n",
        "def relations_1(doc):\n",
        " #sent_tokenize() is used to perform sentence-segmentation\n",
        "  tokenized_sent = nltk.sent_tokenize(doc)\n",
        "  #word_tokenize() is used to tokenize each sentence present in the document into words \n",
        "  tagged_sent = [nltk.word_tokenize(sent) for sent in tokenized_sent]\n",
        "  # POS tagging is performed using pos_tag() \n",
        "  tagged_sent = [nltk.pos_tag(sent) for sent in tagged_sent]\n",
        "#Creating a list to store the results of the \"Produced_By\" relation\n",
        "  produced_res = []\n",
        "  \n",
        "#Defining X \n",
        "#Defining Y\n",
        "#Defining /alpha\n",
        "  for sent in tagged_sent:\n",
        "    subjclass = 'NE'\n",
        "    objclass = 'NE'\n",
        "    \n",
        "    pattern_to_find = re.compile(r'.*produce.*') #For the Produced By relation\n",
        "     #\".* \" in regular expression means \"0 or more of any character\"\n",
        "    #\".\" - a \"dot\" indicates any character\n",
        "    #\"*\" - means \"0 or more instances of the preceding regex token\"\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    chunked = nltk.ne_chunk(sent, binary=True) \n",
        "    pairs = nltk.sem.relextract.tree2semi_rel(chunked)\n",
        "    #Converting the 'semi-relations' that were extracted into a \"semi_rel2reldict\" dictionary\n",
        "    reldicts = nltk.sem.relextract.semi_rel2reldict(pairs + [[[]]])\n",
        "\n",
        "    #\n",
        "    # Extract the  relevant relations that match the regular expression pattern.\n",
        "    relfilter = lambda x: (x['subjclass'] == subjclass and\n",
        "                              pattern_to_find.match(x['filler']) and\n",
        "                              x['objclass'] == objclass)\n",
        "    \n",
        "    rels_list = list(filter(relfilter, reldicts))\n",
        "    for real in rels_list:\n",
        "      print(nltk.sem.relextract.rtuple(real))\n",
        "    # extracting subject and object text based on filtered relations \n",
        "    if len(rels_list) > 0:\n",
        "      produced_res.append(rels_list[0]['objsym'])\n",
        "      produced_res.append(rels_list[0]['subjsym'])\n",
        "  return produced_res\n",
        "\n",
        "relations_1(documents[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "16p4KMlVyQU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4f6fc0-bda3-400c-91bc-f7494e157266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NE: 'American/JJ'] 'epic/NN space-opera/NN film/NN written/VBN and/CC directed/VBN by/IN' [NE: 'George/NNP Lucas/NNP']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['george_lucas', 'american']"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "# relation 2 - your code goes here\n",
        "\n",
        "def relations_2(doc):\n",
        "  #sent_tokenize() is used to perform sentence-segmentation\n",
        "  tokenized_sent = nltk.sent_tokenize(doc)\n",
        "   #word_tokenize() is used to tokenize each sentence present in the document into words \n",
        "  tagged_sent = [nltk.word_tokenize(sent) for sent in tokenized_sent]\n",
        "   # POS tagging is performed using pos_tag() \n",
        "  tagged_sent = [nltk.pos_tag(sent) for sent in tagged_sent]\n",
        "#Creating a list to store the results of the \"Directed_By\" relation\n",
        "  directed_res = []\n",
        "  \n",
        "  ##Defining X \n",
        "#Defining Y\n",
        "#Defining /alpha\n",
        "  for sent in tagged_sent:\n",
        "    subjclass = 'NE'\n",
        "    objclass = 'NE'\n",
        "\n",
        "    pattern_to_find = re.compile(r'.*direct.*') #For the Directed By relation\n",
        "    #\".* \" in regular expression means \"0 or more of any character\"\n",
        "    #\".\" - a \"dot\" indicates any character\n",
        "    #\"*\" - means \"0 or more instances of the preceding regex token\"\n",
        "  \n",
        "\n",
        "    chunked = nltk.ne_chunk(sent, binary=True) \n",
        "    pairs = nltk.sem.relextract.tree2semi_rel(chunked)\n",
        "    #Converting the 'semi-relations' that were extracted into a \"semi_rel2reldict\" dictionary\n",
        "    \n",
        "    reldicts = nltk.sem.relextract.semi_rel2reldict(pairs + [[[]]])\n",
        "\n",
        "    #\n",
        "    # Extract the  relevant relations that match the regular expression pattern.\n",
        "    relfilter = lambda x: (x['subjclass'] == subjclass and\n",
        "                              pattern_to_find.match(x['filler']) and\n",
        "                              x['objclass'] == objclass)\n",
        "    \n",
        "    rels_list = list(filter(relfilter, reldicts))\n",
        "    for real in rels_list:\n",
        "      print(nltk.sem.relextract.rtuple(real))\n",
        "    # extracting subject and object text based on filtered relations \n",
        "    if len(rels_list) > 0:\n",
        "      directed_res.append(rels_list[0]['objsym'])\n",
        "      directed_res.append(rels_list[0]['subjsym'])\n",
        "  return directed_res\n",
        "\n",
        "relations_2(documents[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# relation 3 \n",
        "def relations_3(doc):\n",
        "  #sent_tokenize() is used to perform sentence-segmentation\n",
        "  tokenized_sent = nltk.sent_tokenize(doc)\n",
        "   #word_tokenize() is used to tokenize each sentence present in the document into words \n",
        "  tagged_sent = [nltk.word_tokenize(sent) for sent in tokenized_sent]\n",
        "  # POS tagging is performed using pos_tag() \n",
        "  tagged_sent = [nltk.pos_tag(sent) for sent in tagged_sent]\n",
        "#Creating a list to store the results of the \"Edited_By\" relation\n",
        "  edited_res = []\n",
        "  for sent in tagged_sent:\n",
        "#Defining X \n",
        "#Defining Y\n",
        "#Defining /alpha\n",
        "    subjclass = 'NE'\n",
        "    objclass = 'NE'\n",
        "    pattern_to_find = re.compile(r'.*edited*') #For the Edited By relation\n",
        "    #\".* \" in regular expression means \"0 or more of any character\"\n",
        "    #\".\" - a \"dot\" indicates any character\n",
        "    #\"*\" - means \"0 or more instances of the preceding regex token\"\n",
        "    \n",
        "\n",
        "    chunked = nltk.ne_chunk(sent, binary=True) \n",
        "    pairs = nltk.sem.relextract.tree2semi_rel(chunked)\n",
        "    #Converting the 'semi-relations' that were extracted into a \"semi_rel2reldict\" dictionary\n",
        "    reldicts = nltk.sem.relextract.semi_rel2reldict(pairs + [[[]]])\n",
        "\n",
        "    \n",
        "    # Extract the  relevant relations that match the regular expression pattern.\n",
        "    relfilter = lambda x: (x['subjclass'] == subjclass and\n",
        "                              pattern_to_find.match(x['filler']) and\n",
        "                              x['objclass'] == objclass)\n",
        "    \n",
        "    rels_list = list(filter(relfilter, reldicts))\n",
        "    for real in rels_list:\n",
        "      print(nltk.sem.relextract.rtuple(real))\n",
        "\n",
        "    if len(rels_list) > 0:\n",
        "      edited_res.append(rels_list[0]['objsym'])\n",
        "      edited_res.append(rels_list[0]['subjsym'])\n",
        "  return edited_res\n",
        "\n",
        "relations_3(documents[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6xqnbfHzjTV",
        "outputId": "a7d7d7ff-b531-4035-9ac4-f8ada6c72386"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NE: 'Buena/NNP Vista/NNP International/NNP'] ',/, edited/VBN by/IN' [NE: 'Fabienne/NNP']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fabienne', 'buena_vista_international']"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuJmr-eKvrQ3"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 4: Information / Relation Extraction (II)  (15 Marks)\n",
        "\n",
        "Identify one other relation of your choice, besides the ones mentioned in the previous task, and write a function to extract it. \n",
        "\n",
        "The function you define here must take as input a string called `document` and return the information/relations extracted as a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "kncUM3pHvyAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1582d6d-4d8c-4a8d-f8eb-0dd5b207be54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NE: 'Dark/NNP Knight/NNP'] 'is/VBZ a/DT 2008/CD superhero/NN film/NN directed/VBD ,/, produced/VBN ,/, and/CC co-written/JJ by/IN' [NE: 'Christopher/NNP Nolan/NNP']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['christopher_nolan', 'dark_knight']"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "\n",
        "# relation 4\n",
        "def relations_4(doc):\n",
        "  \n",
        "  #sent_tokenize() is used to perform sentence-segmentation\n",
        "  tokenized_sent = nltk.sent_tokenize(doc)\n",
        "  #word_tokenize() is used to tokenize each sentence present in the document into words \n",
        "  tagged_sent = [nltk.word_tokenize(sent) for sent in tokenized_sent]\n",
        "  # POS tagging is performed using pos_tag() \n",
        "  tagged_sent = [nltk.pos_tag(sent) for sent in tagged_sent]\n",
        "  #Creating a list to store the results of the \"Co-Written\" relation\n",
        "  cowritten_res = []\n",
        "#Defining X \n",
        "#Defining Y\n",
        "#Defining /alpha\n",
        "  \n",
        "  for sent in tagged_sent:\n",
        "    subjclass = 'NE'\n",
        "    objclass = 'NE'\n",
        "    \n",
        "    pattern_to_find = re.compile(r'.*co-written.*')  #For the Co-Written relation\n",
        "    #\".* \" in regular expression means \"0 or more of any character\"\n",
        "    #\".\" - a \"dot\" indicates any character\n",
        "    #\"*\" - means \"0 or more instances of the preceding regex token\"\n",
        "   \n",
        "\n",
        "    chunked = nltk.ne_chunk(sent, binary=True) \n",
        "    pairs = nltk.sem.relextract.tree2semi_rel(chunked)\n",
        "    #Converting the 'semi-relations' that were extracted into a \"semi_rel2reldict\" dictionary\n",
        "   \n",
        "    reldicts = nltk.sem.relextract.semi_rel2reldict(pairs + [[[]]])\n",
        "\n",
        "    # Extract the  relevant relations that match the regular expression pattern.\n",
        "    relfilter = lambda x: (x['subjclass'] == subjclass and\n",
        "                              pattern_to_find.match(x['filler']) and\n",
        "                              x['objclass'] == objclass)\n",
        "    \n",
        "    rels_list = list(filter(relfilter, reldicts))\n",
        "    for real in rels_list:\n",
        "      print(nltk.sem.relextract.rtuple(real))\n",
        "    # extracting subject and object text based on filtered relations \n",
        "    if len(rels_list) > 0:\n",
        "      cowritten_res.append(rels_list[0]['objsym'])\n",
        "      cowritten_res.append(rels_list[0]['subjsym'])\n",
        "  return  cowritten_res\n",
        "\n",
        "relations_4(documents[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIfQCd_Y1x5B"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 5: Combining information in the output (5 Marks)\n",
        "\n",
        "Edit the function below to return a Python dictionary with the outputs from the functions defined in tasks $3 - 4$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHv5pRQ7BKJo"
      },
      "source": [
        "The output from the cell above should look something like the dictionary shown below. Overall values might be different, based on what four items you choose to extract in Tasks 3 and 4, but the structure should be similar.\n",
        "\n",
        "For example, if you choose to extract **Starring**, **Release Date**, **Box office**, and **Directed by**, then the output should look something like this for the first document:\n",
        "\n",
        "```javascript\n",
        "{\n",
        "  'Box office': ['$775 million'],\n",
        "  'Directed by': ['George Lucas'],\n",
        "  'Release date': ['May 25, 1977'],\n",
        "  'Starring': ['Mark Hamill', 'Harrison Ford', 'Carrie Fisher', \n",
        "               'Peter Cushing', 'David Prowse', 'James Earl Jones', ],\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info(document):\n",
        "  '''Extract information and relations from a given document.'''\n",
        "\n",
        "  # Edit the output dict below and assign the values to keys by \n",
        "  # calling the appropriate functions from Tasks 3 and 4.\n",
        "  \n",
        "  # You can delete the keys for which you do not perform extraction in Task 3.\n",
        "  #Except \"Produced by\", \"Directed by\", \"Edited by\" all the other keys are deleted\n",
        "  result_1 = relations_1(document)\n",
        "  result_2 = relations_2(document)\n",
        "  result_3 = relations_3(document)\n",
        "  result_4 = relations_4(document)\n",
        "  output = {\n",
        "   \n",
        "    \n",
        "    # For the relations you extract in Task 3, \n",
        "    # save the output in the appropriate key and delete rest of the keys.\n",
        "    \n",
        "    \"Produced by\": [result_1[0] if len(result_1) > 0 else None  ],\n",
        "    \n",
        "    \"Directed by\": [result_2[0] if len(result_2) > 0 else None ],\n",
        "    \n",
        "    \"Edited by\": [result_3[0] if len(result_3) > 0 else None ],\n",
        "   \n",
        "\n",
        "    # save the output from Task 4 here\n",
        "    \"co-written\": [result_4]\n",
        "\n",
        "    \n",
        "  }\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "# check output for the first document\n",
        "extract_info(documents[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MF81qdyz9Ty",
        "outputId": "7dabb85c-991d-48f4-d504-2f5cbeac1ff2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Directed by': ['christopher_nolan'],\n",
              " 'Edited by': [None],\n",
              " 'Produced by': ['christopher_nolan'],\n",
              " 'co-written': [['christopher_nolan', 'dark_knight']]}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(pred, label):\n",
        "  \n",
        "  output = []\n",
        "  total_predictions = 0\n",
        "  total_labels = 0\n",
        "  counts_num = 0\n",
        "  for val1, val2 in zip(pred, label):\n",
        "    preds = []\n",
        "    labes  = []\n",
        "    for tempo1 in val1:\n",
        "      if tempo1 != None:\n",
        "        tempo1 = tempo1.replace(\"_\", \" \").strip().lower()\n",
        "      else:\n",
        "        tempo1 = None\n",
        "        total_predictions += 1\n",
        "      preds.append(tempo1)\n",
        "\n",
        "    if val2 == None:\n",
        "      val2 = [None]\n",
        "    for tempvar2 in val2:\n",
        "      if tempvar2 != None:\n",
        "        tempo = tempvar2.strip().lower()\n",
        "      else:\n",
        "        tempo1 = None\n",
        "        total_labels += 1\n",
        "      labes.append(tempo1)\n",
        "    output.append((*preds, labes))\n",
        "  \n",
        "  similar = 0\n",
        "  for tres1, tres2 in output:\n",
        "    if tres1 in tres2 and tres1 != None:\n",
        "      similar += 1\n",
        "\n",
        "  p_value = similar / (50 - total_predictions)\n",
        "  r_value = similar / (50 - total_labels)\n",
        "  f1_value = (2 * p_value * r_value) / (p_value + r_value)\n",
        "  return p_value, r_value, f1_value\n",
        "\n",
        "\n",
        "def evaluate(labels, predictions):\n",
        "  '''\n",
        "  Evaluate the performance of relation extraction \n",
        "  using Precision, Recall, and F1 scores.\n",
        "\n",
        "  Args:\n",
        "    labels: A list containing gold-standard labels\n",
        "    predictions: A list containing information extracted from documents\n",
        "  Returns:\n",
        "    scores: A dictionary containing Precision, Recall and F1 scores \n",
        "            for the information/relations extracted in Task 3.\n",
        "  '''\n",
        "\n",
        "  assert len(predictions) == len(labels)\n",
        "  #Defining lists to store the produced by, directed by , edited by entities along with their labels \n",
        "  \n",
        "  produ_predictions,  produ_labels   = [], []\n",
        "  dire_predictions, dire_labels      = [], []\n",
        "  edited_predictions, edited_labels  = [], []\n",
        "\n",
        "  for movie in predictions:\n",
        "    produ_predictions.append(movie['Produced by'])\n",
        "    dire_predictions.append(movie['Directed by'])\n",
        "    edited_predictions.append(movie[\"Edited by\"])\n",
        "\n",
        "\n",
        "  for movie in labels:\n",
        "    produ_labels.append(movie['Produced by'])\n",
        "    dire_labels.append(movie['Directed by'])\n",
        "    if 'Edited by' in list(movie.keys()):\n",
        "      edited_labels.append(movie['Edited by'])\n",
        "    else:\n",
        "      edited_labels.append(None)\n",
        "#The evaluation-function is called in order to compute metrices values for Precision,   Recall and F1 Score  for all the entities\n",
        "  precision_produ_value, recall_produ_value, f1_produ_value    = evaluation(produ_predictions, produ_labels)\n",
        "  precision_dire_value, recall_dire_value, f1_dire_value = evaluation(dire_predictions, dire_labels)\n",
        "  precision_write_value, recall_write_value, f1_write_value = evaluation(edited_predictions, edited_labels)\n",
        "  \n",
        "#Computing the metric values for Precision,   Recall and F1 Score and computing their mean value\n",
        "  scores = {\n",
        "      'Precision': (precision_produ_value + precision_dire_value + precision_write_value)/3,\n",
        "      'Recall': (recall_produ_value + recall_dire_value + recall_write_value)/3,\n",
        "      'F1': (f1_produ_value + f1_dire_value + f1_write_value)/3\n",
        "  }\n",
        "\n",
        "  # calculate the precision, recall and f1 score over the information fields \n",
        "  # corresponding to Task 3 and store the result in the `scores` dict.\n",
        "\n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "asapaS0D5441"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDMhFQq4fBnf"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 6: Evaluation (I) (15 Marks)\n",
        "\n",
        "Write a function to evaluate the performance of Task $3$ using **Precision**, **Recall** and **F1** scores. Use the gold-standard labels provided in the JSON files to calculate these values.\n",
        "\n",
        "Please note that not all the information / relations mentioned in Task $3$ have associated labels for each and every movie in the JSON documents, i.e., some JSON documents will have certain keys-value pairs missing. For example, we have labels for *Budget* in 46 out of the 50 movies and in the remaining 4 documents, you will find that the key `Budget` is omitted from the JSON.\n",
        " \n",
        "Also keep in mind that we will further run this evaluation on a hidden test set containing similar movie descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA39EbBCfRu6"
      },
      "source": [
        "---\n",
        "Run the cell below to calculate and display the evaluation scores for the 50 documents in `movies.zip`.\n",
        "\n",
        "You can consider the following as a baseline score. Your aim should be to score higher or atleast get as close as possible to these values.\n",
        "\n",
        "| Precision | Recall | F1    |\n",
        "| :---:     | :---:  | :---: |\n",
        "| 0.5       | 0.25   | 0.333 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "CRxOd4dIfRu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "dfa62423-9574-49c7-fb8c-8f5f0c0cce93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-33f1a268-a440-418e-9f06-10ff136fa9f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500816</td>\n",
              "      <td>0.609041</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33f1a268-a440-418e-9f06-10ff136fa9f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33f1a268-a440-418e-9f06-10ff136fa9f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33f1a268-a440-418e-9f06-10ff136fa9f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Precision    Recall        F1\n",
              "0        1.0  0.500816  0.609041"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "# calculate evaluation score across all the 50 documents\n",
        "extracted_infos = []\n",
        "for document in documents:\n",
        "  extracted_infos.append(extract_info(document))\n",
        "\n",
        "scores = evaluate(labels, extracted_infos)\n",
        "\n",
        "pd.DataFrame([scores])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agNQPVqG5aoS"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 7: Evaluation (II) (10 Marks)\n",
        "\n",
        "Describe **two** challenges you encountered above or might encounter in the evaluation of *information extraction* or *relation extraction* tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   The related words may not appear in all documents. This may differ from one document to the next, and it may have an impact on the rating matrices. Choosing the related words was also difficult because they had to be chosen based on their frequency in the majority of the documents.\n",
        "2.   The majority of these related words had word cases that were mixed. This could potentially have an impact on the prediction process. As a result, the (upper or lower)cases can be  ignored using re.IGNORECASE and the procedure can be continued.\n"
      ],
      "metadata": {
        "id": "N7sh2ftnL-ul"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "21235396_BENGALURU RAGHURAM .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}